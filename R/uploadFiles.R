#' Utilities for artifact upload
#'
#' Utilities for uploading files to an ArtifactDB instance, used internally by \code{\link{uploadProject}}.
#'
#' @param dir String containing the path to a project directory on the file system, containing files to be uploaded.
#' @param files Character vector of paths to files to be uploaded within \code{dir}.
#' These should be relative to \code{dir}.
#' @param dedup.link Named character vector specifying which files are duplicates of resources in existing ArtifactDB IDs. 
#' Each name in the vector should be a relative path to a file in \code{dir}, while each value of the vector should be the existing ArtifactDB ID.
#' Names should not overlap with entries in \code{files}.
#' @param start.url String containing the URL to the upload endpoint.
#' @param initial The response object returned by \code{initializeUpload}, or a list generated by calling \code{\link{content}} on the response.
#' @param auto.dedup.md5 Logical scalar indicating whether to check \code{files} for duplicates across versions based on their MD5 checksums, see Details.
#' @param md5.field String specifying the field containing the MD5 checksums.
#' @param dedup.md5 Named character vector containing the MD5 sums of files that are potentially duplicated in the ArtifactDB backend.
#' Each name in the vector should be a relative path to a file in \code{dir}, while each value of the vector should be its MD5 sum.
#' Names should not overlap with entries in \code{files}.
#' @param permissions A list containing permission information, see \code{\link{getPermissions}}.
#' This usually contains the \code{owners} and \code{viewers} character vectors.
#' @param overwrite.permissions Logical scalar indicating whether existing permissions should be overwritten.
#' @param expires Integer scalar specifying the number of days before the files expire and are removed from the ArtifactDB instance.
#' By default, the files have no expiry date.
#' @param attempts Integer scalar specifying the number of upload attempts on each file, in case of connection drops or timeouts.
#' Each attempt is followed by a 1 minute wait.
#' @param url String containing the URL to the REST API.
#' @param index.wait Numeric scalar specifying the number of seconds to wait when checking for correct indexing.
#' @param must.index Logical scalar indicating whether to throw an error if the indexing is not successful.
#' If \code{FALSE}, a warning is raised if the indexing times out (but an error will still be raised if the indexing explicitly fails).
#' @param project String containing the project name.
#' @param version String containing the version.
#' @param user.agent String containing a user agent string.
#' If \code{NULL}, a default user agent is used.
#' 
#' @return
#' \code{initializeUpload} will return the \code{response} object from hitting the upload endpoint.
#' This will have already been checked for failure.
#' If parsed as JSON, this produces a list with either presigned URL or link URL for each file, depending on whether deduplication was requested and successful.
#' 
#' \code{uploadFiles} will upload all files to their URLs and return \code{NULL}.
#'
#' \code{requestCompletion} will return the \code{response} object from hitting the completion endpoint.
#' This will have already been checked for failure.
#'
#' \code{abortUrl} will return the \code{response} object from hitting the abort endpoint.
#' This will \emph{not} have been checked for failure, not least because this function is typically called in response to other errors during the upload;
#' we leave it to the caller to decide whether to add another error to the trace.
#'
#' \code{createUploadStartUrl} will create the \dQuote{standard} upload URL to be used in \code{start.url}.
#'
#' @section Linking to duplicate files:
#' ArtifactDB instances are capable of creating links to represent identical files, similar to symbolic links on a typical file system.
#' This avoids the need to explicitly store a duplicate copy of a file in another project or version.
#' It is particularly useful when dealing with new project versions where only a subset of files have changed.
#'
#' The first deduplication mechanism is implicit and based on the MD5 checksums of the uploaded (non-metadata) files. 
#' When \code{auto.dedup.md5=TRUE} or \code{dedup.md5} is supplied, the ArtifactDB backend will check the most recent previous version of the project (if any exists) for files with the same path and checksum.
#' If such files are found, the backend will automatically create a link to that file in the previous version, avoiding the need to upload a new copy in \code{uploadFiles}.
#' \code{auto.dedup.md5} will extract/compute the MD5 checksums from \code{files}, while \code{dedup.md5} allows users to specify the MD5 checksum per file manually if this is so desired.
#'
#' The second mechanism is more explicit, relying on the uploader to supply the ArtifactDB identifier of the existing file to be linked to.
#' This involves a bit more work on the part of the uploader but is more powerful than the MD5-based method as it can be used to link files across different projects.
#' Linked files can either be specified directly via \code{dedup.link} or they can be detected from \code{files} as placeholder symlinks generated by \code{\link{createPlaceholderLink}}.
#'
#' Developers can check whether links were successfully created by inspecting the \code{\link{content}} of the \code{initializeUpload} output.
#' If the to-be-linked file appears in the \code{links} field, the link was created; otherwise, the file must be uploaded via the \code{presigned_urls}.
#' This is useful for double-checking that the MD5 sums were correctly recognized by the API. 
#'
#' @details
#' Use of these utilities will almost always require appropriate authentication/authorization with the target API.
#' Developers should ensure that \code{\link{identityHeaders}} and friends are set accordingly.
#'
#' Setting \code{expires} in \code{initializeUpload} is useful for testing the upload procedure without creating a permanent copy of the files.
#' Project versions created with \code{expires} will be automatically removed after the expiry interval, freeing up space for real data.
#' 
#' @examples
#' # Creating a mock project.
#' src <- system.file("scripts", "mock.R", package="zircon")
#' source(src)
#' tmp <- tempfile()
#' createMockProject(tmp)
#' 
#' f <- list.files(tmp, recursive=TRUE)
#' f
#' start.url <- createUploadStartURL(example.url, "test-zircon-upload", "base2")
#' start.url
#'
#' # Basic upload sequence, for testing:
#' \dontrun{
#' init <- initializeUpload(tmp, f, start.url, expires=1)
#' parsed <- httr::content(init)
#' uploadFiles(tmp, example.url, parsed)
#' completeUpload(example.url, parsed)
#' }
#'
#' # Demonstrating how to create links:
#' actual.files <- f[!endsWith(f, ".json")]
#' md5 <- digest::digest(file=file.path(tmp, actual.files[1]))
#' names(md5) <- actual.files[1]
#'
#' explicit <- packID(example.project, actual.files[2], example.version)
#' names(explicit) <- actual.files[2]
#'
#' \dontrun{
#' start.url <- createUploadStartURL(example.url, "test-zircon-upload", "relinked")
#' init <- initializeUpload(tmp, start.url=start.url, expires=1,
#'     files=setdiff(f, c(names(md5), names(explicit))),
#'     dedup.md5=md5, dedup.md5.field="md5sum", dedup.link=explicit)
#' parsed <- httr::content(init)
#' uploadFiles(tmp, example.url, parsed)
#' completeUpload(example.url, parsed)
#' }
#'
#' @seealso
#' \code{\link{uploadProject}}, for a more user-friendly wrapper around these utilities.
#'
#' @author Aaron Lun
#' @export
#' @rdname upload-utils
#' @importFrom httr POST add_headers 
initializeUpload <- function(dir, files, start.url, auto.dedup.md5=FALSE, dedup.md5=NULL, md5.field="md5sum", dedup.link=NULL, expires=NULL, user.agent=NULL) {
    stopifnot(length(intersect(files, names(dedup.link)))==0L)
    stopifnot(length(intersect(files, names(dedup.md5)))==0L)
    stopifnot(length(intersect(names(dedup.link), names(dedup.md5)))==0L)

    files <- .format_files(dir, files, auto.dedup.md5=auto.dedup.md5, md5.field=md5.field)

    # Explicitly listed MD5 for deduplication.
    md5.files <- vector("list", length(dedup.md5))
    for (i in seq_along(md5.files)) {
        fname <- names(dedup.md5)[i]
        .check_file_size(dir, fname)
        md5.files[[i]] <- list(filename=fname, check="md5", value=list(field=md5.field, md5sum=dedup.md5[[i]]))
    }

    # Explicit links. No need to check the file size here, as we're not actually uploading it.
    link.files <- vector("list", length(dedup.link))
    for (i in seq_along(link.files)) {
        link.files[[i]] <- list(filename=names(dedup.link)[i], check="link", value=list(artifactdb_id=dedup.link[[i]]))
    }

    body <- list(
        filenames = c(files, md5.files, link.files),
        mode = "s3-presigned-url"
    )
    if (!is.null(expires)) {
        body$expires_in <- paste("in", expires, "days")
        body$completed_by <- body$expires_in # this needs to be <= expiry date.
    }

    added <- .follow_redirects_faithfully(POST, start.url, body=body, encode='json', user.agent=user.agent)
    checkResponse(added)

    added
}

#' @importFrom jsonlite fromJSON
.format_files <- function(dir, files, auto.dedup.md5, md5.field) {
    link.targets <- Sys.readlink(file.path(dir, files))
    is.link <- link.targets != ""
    files <- as.list(files)

    # Extracting the links.
    for (f in which(is.link)) {
        files[[f]] <- list(filename=files[[f]], check="link", value=list(artifactdb_id=.extract_link_id(link.targets[[f]])))
    }

    # Extracting and/or computing MD5 for integrity and/or deduplication.
    for (f in which(!is.link)) {
        fname <- files[[f]]
        .check_file_size(dir, fname)

        current.dedup.md5 <- auto.dedup.md5
        if (!endsWith(fname, ".json")) {
            meta.path <- file.path(dir, paste0(fname, ".json"))
            md5 <- fromJSON(meta.path)[[md5.field]]
        } else {
            # This should only be necessary for the metadata documents.
            md5 <- digest::digest(file=file.path(dir, fname))
            current.dedup.md5 <- FALSE
        }

        base <- list(filename=fname, value=list(md5sum=md5))
        if (!current.dedup.md5) {
            base$check <- "simple"
        } else {
            base$check <- "md5"
            base$value$field <- md5.field
        }
        files[[f]] <- base
    }

    files
}

#' @export
#' @rdname upload-utils
createUploadStartURL <- function(url, project, version) {
    paste0(url, "/projects/", project, "/version/", version, "/upload")
}

.check_file_size <- function(dir, f) {
    full <- file.path(dir, f)
    fsize <- file.info(full)$size
    gig <- 2^30
    limit <- min(5, getOption("ArtifactDB.upload.size.limit", 1))
    if (fsize >= limit * gig) {
        stop(paste(strwrap(paste0("refusing to upload '", full, "', which is over ", limit, " GB in size - see ?.uploadFiles for details"), 80), collapse="\n"))
    }
}

#' @importFrom httr content
.parse_initial <- function(initial) {
    if (is(initial, "response")) {
        content(initial, simplifyVector=TRUE, simplifyMatrix=FALSE, simplifyDataFrame=FALSE)
    } else {
        initial
    }
}

#' @export
#' @rdname upload-utils
#' @importFrom httr PUT stop_for_status upload_file add_headers content
uploadFiles <- function(dir, url, initial, user.agent=NULL, attempts=3) {
    dedup.urls <- .parse_initial(initial)$links
    for (d in seq_along(dedup.urls)) {
        current <- dedup.urls[[d]]
        endpoint <- paste0(url, "/", current$url)
        out <- .follow_redirects_faithfully(PUT, endpoint, user.agent=user.agent)
        checkResponse(out)
    } 

    # Looping through all files and uploading them. Each upload undergoes several 
    # attempts to be robust to connection loss or timeouts.
    up.urls <- initial$presigned_urls
    for (g in seq_along(up.urls)) {
        current <- up.urls[[g]]
        up.url <- current$url
        up.md5 <- current$md5sum
        up.path <- current$filename
        failed <- TRUE
        msg <- NULL

        for (i in seq_len(attempts)) {
            out <- PUT(up.url, body=upload_file(file.path(dir, up.path)), .user_agent(user.agent), add_headers(`Content-MD5`=up.md5))
            if (out$status_code < 300) {
                failed <- FALSE
                break
            }

            msg <- try(content(out, as="text", encoding="UTF-8"))
            if (i < attempts) {
                # Try again after some time, maybe it's feeling better.
                Sys.sleep(60)
            }
        }

        if (failed) {
            err <- paste0("failed to upload '", up.path, "'")
            if (!is.null(msg) && !is(msg, "try-error")) {
                err <- paste0(err, ":\n", msg)
            }
            stop(err)
        } 
    }
}

#' @export
#' @rdname upload-utils
#' @importFrom httr PUT add_headers 
completeUpload <- function(url, initial, index.wait=600, must.index=FALSE, permissions=list(), overwrite.permissions=FALSE, user.agent=NULL) {
    initial <- .parse_initial(initial)
    end.url <- paste0(url, initial$completion_url)

    if (overwrite.permissions) {
        end.url <- paste0(end.url, "&overwrite_permissions=true")
    }

    if (is.null(names(permissions))){
        names(permissions) <- seq_along(permissions)
    }
    if (!is.null(permissions$viewers)) {
        permissions$viewers <- I(permissions$viewers)
    }
    if (!is.null(permissions$owners)) {
        permissions$owners <- I(permissions$owners)
    }

    if (is.null(end.url)) {
        end.url <- initial$completion_url
    }

    fin <- .follow_redirects_faithfully(PUT, end.url, body=permissions, encode="json", user.agent=user.agent)
    checkResponse(fin)

    guts <- content(fin)
    status.url <- paste0(url, "/jobs/", guts$job_id)
    okay <- failed <- FALSE 

    counter <- 5 
    while (index.wait > 0) {
        Sys.sleep(counter)
        index.wait <- index.wait - counter
        X <- GET(status.url)
        if (X$status_code < 300) {
            index.info <- content(X)
            index.status <- index.info$status
            if (index.status == "SUCCESS") {
                okay <- TRUE
                break
            } else if (index.status == "FAILURE") {
                failed <- TRUE
                break
            }
        }
    }

    if (!okay) {
        if (failed || must.index) {
            stop(paste(strwrap(paste0("indexing failure for job '", guts$job_id, "', see ", status.url, " for more details"), 80), collapse="\n  "))
        } else {
            warning(paste(strwrap(paste0("indexing timeout for job '", guts$job_id, "', see ", status.url, " for more details"), 80), collapse="\n  "))
        }
    }

    fin
}

#' @export
#' @rdname upload-utils
#' @importFrom httr PUT
abortUpload <- function(url, initial, user.agent=NULL) {
    initial <- .parse_initial(initial)
    abort.url <- paste0(url, initial$abort_url)
    .follow_redirects_faithfully(PUT, abort.url, user.agent=user.agent)
}
